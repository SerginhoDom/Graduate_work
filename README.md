# Отчёт об архитектуре KAN (Kolmogorov-Arnold Networks)

## Введение
KAN (Kolmogorov-Arnold Networks) — это новая архитектура нейронных сетей, основанная на теореме Колмогорова-Арнольда о представлении многомерных функций. Она была предложена как альтернатива традиционным многослойным перцептронам (MLP), с целью улучшения интерпретируемости и эффективности в задачах машинного обучения. В отличие от классических нейронных сетей, KAN использует не фиксированные функции активации, а обучаемые одномерные функции, что делает её более гибкой и адаптивной.

## Основы теоремы Колмогорова-Арнольда
Теорема Колмогорова-Арнольда утверждает, что любая непрерывная функция нескольких переменных может быть представлена как суперпозиция конечного числа непрерывных функций одной переменной. Формально для функции \( f: [0,1]^n \to \mathbb{R} \) существует представление:

\[ f(x_1, x_2, ..., x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right), \]

где:
- \( \phi_{q,p} \) — одномерные непрерывные функции,
- \( \Phi_q \) — внешние функции, также одномерные.

KAN использует эту идею, заменяя традиционные нейроны с фиксированными активациями (например, ReLU или сигмоидой) на обучаемые функции.

## Архитектура KAN
Архитектура KAN состоит из следующих ключевых компонентов:

### 1. Входной слой
Входной слой принимает многомерные данные \( x = (x_1, x_2, ..., x_n) \), где \( n \) — размерность входа. Каждый входной признак передаётся в сеть без предварительной обработки.

### 2. Слой одномерных функций
Вместо линейной комбинации весов и фиксированной активации, как в MLP, KAN применяет к каждому входному признаку \( x_i \) набор обучаемых одномерных функций \( \phi_{q,i}(x_i) \). Эти функции обычно параметризуются с помощью сплайнов (например, B-сплайнов), что позволяет модели гибко подстраиваться под данные.

- **Параметры сплайнов**: Коэффициенты сплайнов обучаются в процессе оптимизации, что делает KAN более выразительной по сравнению с фиксированными функциями активации.
- **Количество функций**: Для каждого входа создаётся \( 2n+1 \) внутренних функций, что соответствует теореме Колмогорова-Арнольда.

### 3. Суммирование
На следующем этапе результаты всех одномерных функций \( \phi_{q,i}(x_i) \) суммируются по каждому индексу \( q \):

\[ s_q = \sum_{i=1}^n \phi_{q,i}(x_i). \]

Это формирует промежуточные представления, которые затем передаются дальше.

### 4. Внешние функции
Каждое промежуточное значение \( s_q \) пропускается через внешнюю одномерную функцию \( \Phi_q(s_q) \), которая также является обучаемой (например, параметризованной сплайнами). Итоговый выход получается как сумма:

\[ y = \sum_{q=1}^{2n+1} \Phi_q(s_q). \]

### 5. Выходной слой
Выходной слой выдаёт окончательный результат \( y \), который может быть скаляром (для регрессии) или вектором (для классификации, с добавлением соответствующего преобразования, например, softmax).

## Преимущества KAN
- **Интерпретируемость**: Благодаря использованию одномерных функций, KAN позволяет легче понять вклад каждого признака в итоговый результат.
- **Гибкость**: Обучаемые сплайны делают модель более адаптивной к сложным зависимостям в данных.
- **Эффективность**: KAN может достигать высокой точности с меньшим количеством параметров по сравнению с глубокими MLP.

## Недостатки
- **Вычислительная сложность**: Обучение сплайнов требует больше вычислительных ресурсов, чем применение фиксированных функций активации.
- **Ограниченная масштабируемость**: Пока KAN тестировалась преимущественно на небольших задачах, её поведение на больших наборах данных требует дальнейшего изучения.
