# Архитектура KAN (Kolmogorov-Arnold Networks)

## Введение
KAN (Kolmogorov-Arnold Networks) — это современная архитектура нейронных сетей, вдохновлённая теоремой Колмогорова-Арнольда. Она предлагает альтернативу традиционным многослойным перцептронам (MLP), улучшая интерпретируемость и эффективность. В отличие от MLP, где используются фиксированные функции активации, KAN применяет обучаемые одномерные функции, что повышает её гибкость.

## Теорема Колмогорова-Арнольда
Теорема утверждает, что любая непрерывная функция от $n$ переменных может быть представлена через сумму одномерных функций. Математически это выражается как:

$$
f(x_1, x_2, ..., x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right),
$$

где $ \phi_{q,p} $ — внутренние одномерные функции, а $ \Phi_q $ — внешние одномерные функции. KAN использует эту концепцию для замены фиксированных активаций.

## Структура KAN
Архитектура KAN включает несколько этапов обработки данных:

### Входной слой
Сеть принимает входные данные $ x = (x_1, x_2, ..., x_n) $, где $ n $ — число признаков. Никакой предварительной обработки не требуется.

### Слой одномерных функций
Каждый вход $ x_i $ преобразуется через набор обучаемых функций $ \phi_{q,i}(x_i) $. Эти функции часто реализуются как сплайны (например, B-сплайны), параметры которых оптимизируются во время обучения. Для каждого входа создаётся $ 2n+1 $ таких функций.

### Суммирование
Результаты внутренних функций суммируются для каждого $ q $:

$$
s_q = \sum_{i=1}^n \phi_{q,i}(x_i).
$$

Это создаёт промежуточные значения, передаваемые дальше.

### Внешние функции
Каждое $ s_q $ преобразуется через внешнюю функцию $ \Phi_q(s_q) $, также обучаемую (например, через сплайны). Итоговый выход вычисляется как:

$$
y = \sum_{q=1}^{2n+1} \Phi_q(s_q).
$$

### Выходной слой
Выход $ y $ может быть скаляром (для регрессии) или вектором (для классификации с дополнительным преобразованием, например, softmax).

## Преимущества
- **Интерпретируемость**: Одномерные функции упрощают анализ вклада каждого признака.
- **Гибкость**: Сплайны адаптируются к сложным зависимостям.
- **Эффективность**: Меньше параметров для аналогичной точности по сравнению с MLP.

## Недостатки
- **Сложность вычислений**: Обучение сплайнов требует больше ресурсов.
- **Масштабируемость**: Эффективность на больших данных ещё изучается.
